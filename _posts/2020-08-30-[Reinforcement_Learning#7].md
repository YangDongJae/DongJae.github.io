---
  title : "강화학습, 다이내믹 프로그래밍의 한계"

  describe : "강화학습 과 다이내믹 프로그래밍 , 어디까지?"

  categories : 
      AI

  toc : True

  toc_label: "목차"

  tags : 
    AI
    강화학습

  last_modified_at : 2020-08-30

  use_math: true
---

# Dynamic Programming의 한계
* **Dynamic Programming은 계산을 빠르게 하는 것이지 "_학습_"을 하는것이 아니므로 머신러닝이 아님.**

## 계산 복잡도

* Dynamic Programming의 계산 복잡도는 상태 크기의 3제곱에 비례함 즉 간단한 문제가 아닌 경우의수가 많은 ( 스타 , 바둑 , 로봇 등)의 문제를 절대 풀 수 없음

## 차원의 저주 (Curse of Dimentionality)

* 상태의 차원이 늘어나면 상태의 수가 재수적으로 증가함 ($S^d$) 

## 환경에 대한 정보의 부족

* Dynamic Programming은 보상과 상태 변환 확률을 정확히 알고있어야함. 하지만 대다수의 상황에서는 이 정보를 정확히 알 수 없음.

## 강화학습의 등장
**위 한계를 극복하기위해 환경을 모르지만 환경과의 상호작용을 통해 경험을 바탕으로 학습하는 _강화학습_이 등장함**

### 환경의 모델

---

#### 모델이란?
* 모델이란 시스템에 입력이 들어왔을 때 시스템이 어떤 출력을 내는지에 대한 방정식

> 환경의 모델 <br><br> 
$P^a_ss',r(s,a)$

#### 모델링이란? 
* 모델링이란 입력과 출력의 관계를 식으로 나타내는 과정


---


게임에서는 사람이 환경을 만들고, 사람이 정해준 대로만 게임이 작동하기 때문에 모델링 오차는 없지만, 실제에서는 환경의 모델을 정확히 알기 어려움.

**모델을 정확히 알기 어려운 경우에 시스템의 입력과 출력 사이의 관계를 알기 위해 두가지방법으로 접근함**


1. 할 수 있는 선에서 정확한 모델링을 한 후 모델링 오차에 대한 부분을 실험을 통해 조정함.
* 학습의 개념이 없음
* 고전적으로 많이 적용함
* 시스템의 안정성을 보장함
* **문제가 복잡해지고 어려워질수록 한계에 봉착함**

2. 모델 없이 환경과의 상호작용을 통해 입력과 출력 사이의 관계를 학습함(강화학습)
* 학습의 특성상 모든 상황에서 동일하게 작동한다고 보장할 수 없음
* **많은 복잡한 문제에서 모델이 필요없음**


