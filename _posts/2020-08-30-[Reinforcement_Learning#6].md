---
  title : "강화학습 가치 이터레이션이란?"

  describe : "강화학습 가치 이터레이션이란? Value Iteration"

  categories : 
      AI

  toc : True

  toc_label: "목차"

  tags : 
    AI
    강화학습

  last_modified_at : 2020-08-30

  use_math: true
---
# 가치 이터레이션 (Value Iteration)

> 가치 이터레이션 (Value Iteration)은 현재의 가치함수가 최적이라고 가정하고 그 가치함수에 대해 결정적인 형태의 정책을 적용시킨 후 계산한다. <br><br> 가치 이터레이션에서는 정책 이터레이션에서처럼 명시적으로 정책이 표현되는 것이 아니고 가치함수 안에 내재적으로 포함돼 있기  때문에, 가치함수를 업데이트하면 자동으로 정책 또한 발전된다.

## 벨만 최적 방정식과 가치 이터레이션

우리는 벨만 최적 방정식을 통해 최적 가치함수를 도출함 순서는 아래와 같음

1. 가치함수를 최적 정책에 대한 가치함수라고 가정함
2. 반복적으로 계산함 
> Question Point <br> <br> 무엇을? 벨만 최적방정식을?
3. 결국 최적 정책에 대한 참 가치함수 , 즉 최적가치함수를 찾음

**가치 이터레이션에서는 시작부터 최적 정책을 가정했기 때문에 한 번의 정책 평가과정을 거치면 최적 가치함수와 최적 정책이 구해족, MDP가 풀림**<br><br>

> 벨만 최적 방정식은 max값을 취학 때문에, 새로운 가치함수를 업데이트 할 때 정책의 값을 고려해줄 필요가 없음. 즉 벨만 최적 방정식에서는 그저 현재 상태에서 가능한 $R_{t+1} + \gamma v_k(S_{t+1})$의 값들 중에서 최고의 값으로 업데이트함<br>$$v_{k+1} = \displaystyle \max_{a\in A}(r_{(s,a)} + \gamma v_k(s'))$$

* 가치 이터레이션은 정책 이터레이션과 다르게 평ㄱ, 정책발전으로 단계가 나누어있지 않음.

* 가치 이터레이션은 현재의 가치함수가 최적 정책에 대한 가치함수라고 가정하기 때문에 정책을 발전하는 함수가 따로 필요하지 않음.

* 벨만 최적 방정식을 통해 구한 가치함수를 토대로 에이전트는 자신이 할 행동을 구할 수 있음.

* 탐욕 정책 (Greedy Policy)를 위해서는 큐함수를 비교해야 하므로 모든 행동에 대해 다음 코드를 실행해 큐 함수를 구함
* * ⚠️만약 가장 큰 value를 갖는 행동이 여러개이 시 하나의 리시트로 그 행동을 모두 담음