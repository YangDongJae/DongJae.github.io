---
  title : "강화학습 , MDP란?"

  describe : "강화학습 , MDP란? , 파이썬과 케라스로 배우는 강화학습"

  categories : 
      AI

  toc : True

  toc_label: "목차"

  tags : 
    AI
    강화학습

  last_modified_at : 2020-08-02

  use_math: true
---
# 벨만 방정식
강화학습이 결국 어떠한 방정식을 풀어내는 방법이라고 가정했을 때, 그 방정식은 벨만 방정식 임

## MDP
강화학습을 위해서는 문제정의가 가장 중요하다. 학습하기에 많지도 않고 적지도 않은 적절한 정보를 에이전트가 알 수 있도록 문제를 잘 정의해야 한다.

### 구성요소
#### 상태
$S$
라고 표기하며 에이전트가 자신의 상황에 대한 관찰
* 실제세상에서의 에이전트에게 상태는 센서 값 
* 게임을 학습하기 위한 에이전트는 사용자가 상태를 정의해 주어야 함.

$ S_t = s$
특정시간 t에서의 상태 St가 어떤상태 s 다.
* t 는 특정 시간을 의미
* S 는 확률변수
#### 행동
A = 에이전트가 상태 St에서 할 수 있는 가능한 행동의 집합

$A_t = a$
At는 어떤 t라는 시간에 집합 A에서 선택한 행동.<br>
에이전트가 어떤 행동을 할지 정해져 있는 것이 아니므로 At와 같이 대무낮로 표현함 즉 A는 확률변수임.

#### 보상 함수
$r(s,a) = E[R_{t+1}|S_t = s , A_t = a]$
보상함수 수식 
<br>
**시간t일 때 상태가 St = s 이고 그 상테에서 행동 At = a 를 했을 경우에 받을 보상에 대한 기댓값 E**
* 기댓값이란 일종의 평균
* 기대값은 어떤 정확한 값이 아니라 나오게 될 숫자에 대한 예상
* 기댓값의 수식은 대문자 E로 표기
* 보상함수를 기댓값으로 표현하는 이유는 보상을 에이전트에게 주는것은 환경이고 환경에 따라서 같은 상태에서 같은 행동을 취하더라도 다른 보상을 줄 수도 있기 때문에
* 보상을 받는 것이 t + 1  인 이뉴는 보상을 에이전트가 알고 있는 것이 아니고 환경이 알려주는 것이기 떄문임. 
* 에이전트는 환경으로부터 하나의 시간 단위가 지난 다음에 보상을 받고 이를 Time step 이라고 함.
#### 상태 변환 확률
$s'$
다음 타임스텝에 에이전트가 갈 수 있는 어떤 특정한 상태를 의미함.

* 상태의 변화에는 확률적인 요인이 들어가고 이를 수치적으로 표현한 것이 상태변환 확률
* 상태 변환 확률 수식
> $ P^a_{ss} = P[S_{t+1} = s'|S_t = s, A_t = a] $
p는 확률을 의미하며, 상태변환 확률은 상태 s에서 a행동을 취했을 때 다른 상태 s'으로 도달할 확률값
* 상태 변환 확률은 환경의 모델이라고 부르며, 환경은 에이전트가 행동을 취하면 상태 변환 확률을 통해 다음에 에이전트가 갈 상태를 알려줌

#### 할인율
현재에 가까운 보상일수록 더 큰 가치를 가진다. 은행의 이자를 생각해보면 현재 와 미래에 같은 가치의 1억 임에도 불구하고 미래에는 '이자'를 통해 더 높은 가치를 반환한다.
<br>
즉 같은 보상이면 나중에 받을수록 가치가 줄어들기 때문에 우리는 이를 수학적으로 표현하기위하여 **할인율**이라는 개념을 도입한다.

* 미래의 가치를 현재의 가치로 환산하는 것을 할인한다고 하고, 시간에 따라 할이한느 비율을 할인율 이라고한다.
* $\gamma \in [0,1]$
즉 할인율 감마는 0~1사이의 값을 갖는다.
* 할인율을 고려한 미래 보상의 현재 가치 , 즉 현재의 시간 t 로부터 시간 k가 지난 후에 보상을 Rt+k를 받을것이라고 하면 수식은 아래와 같다.
$ \gamma^{k-1}R_{t+k}$

#### 정책
정책은 모든 상태에서 에이전트가 할 행동으로, 정책은 각 상태에서 하나의 행동만을 나타낼 수도 있고 , 확률적으로 여러가지 행동을 나타낼 수 있다.

* 에이전트가 강화학습을 통해 학습해야 할 것은 수많은 정책 중에서 최적 정책임
* 정책의 수식은 아래와 같다.
$ \pi(a|s) = P[A_t = a | S_t = s]$
> 시간 t에 St = s에 에이전트가 있을 때 가능한 행동 중에서 At = a를 할 확률
* 에이전트가 현재 상태에서 앞으로 받을 보상들을 고려해서 행동을 결정하면 환경은 에이전트에게 실제 보상과 다음 상태를 알려줌.
* 위 과정을 반복하면서 에이전트는 어떤 상태에서 앞으로 받을 것 이라 예상했던 보상에 대해 틀렸다는 것을 알게됨

#### 가치함수
에이전트가 앞으로 받을 보상들에 대한 개념
>  MDP -> 가치함수 -> 행동선택

$ R_{t+1} + R_{t+2} + R_{t+3} + R_{t+4} +...$
일련 보상들의 단순합<br>
**보상은 행동을 했을 떄가 아닌 그다음 타임스텝에 받는다. 그래서 시간 t 에 행동을 해서 받는 보상은 Rt+1이다.**

##### 가치함수의 문제
1. 에이전트 입장에서는 지금받은 보상이나 미래에 받는 보상이나 똑같이 취급함. 할인하지 않았다면 에이전트가 보게되는 보상의 합은 단순한 덧셈이기 때문에.

2. 100이라는 보상을 1번 받는 것과 20이라는 보상을 5번 받는 것을 구분할 방법이 없다.

3. 시간이 무한대라고 하면 보상을 시간마다 0.1씩 받아도 합이 무한대고, 1씩 받아도 합이 무한대 이다. 수치적으로 두 경우를 구분하지 못한다.

에이전트는 위에서 본바와같이 ㅅ단순한 보상의 합으로는 시간t에있었던 상태가 어떤 가치를 가지는지 판단하기 어렵기 때문에, 좀더 정확하게 상태의 가치를 판단하기 위해 할인율 이라는 개념을 사용한다.

>할인율을 적용한 보상들의 합 수식
$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ...$

할인율을 적용한 보상들의 합을 우리는 Gt 라고하며, **반환값** 이라고한다.
<br>
반환값은 에이전트가 실제로 환경을 탐험하며 받은 보상의 합이며, 유한한 에이전트와 환경의 상호작용을 **에피소드**라고 부른다.
<br>
에피소드가 끝난 후에 "그때로부터 얼마의 보상을 받았지?" 라고 반환값을 계산할 수 있다.

<br>
MDP로 정의되는 세계에서 에이전트와 환경의 상호작용은 불확실성을 내포하고 있기 때문에 특정 상태의 반환값은 에피소드마다 다를 수 있다.

<br>
에이전트는 특정 상태의 가치를 판단하기위해 반환값에 대한 기댓값으로 특정 상태의 가치를 판단한다.

> 가치함수 
$v(s) = E[G_t|S_t = s]$

<br>
E = 기댓값 , G = 반환값 , $S_t$ = 시간에 따른 상태 (확률변수) , s = 상태 <br>
각 타입스텝마다 받는 보상이 모두 확률적이고 반환값이 그 보상들의 합이므로 반환값은 확률변수임.<br>
**하지만 가치함수는 확률변수가 아니라 특정 양을 나타내느 값이므로 소문자로 표현 , 가치를 고려하느느 이유는 만약 현재 에이전트가 갈 수 있는 상태들의 가치를 안다면 그중에서 가장 가치가 제일 높은 상태를 선택할 수 있기 때문임** 마치 사람도 어떤 선택을 할 떄 여러 선택을 놓고서 '이게 괜찮을 것 같은데 ' 하면서 각 선택이 가져올 가치의 기대값을 따지는 것 처럼.
<br>

에이전트는 위 개념인 가치함수를 통해 어느 상태가 좋을지 판단하며, 가치함수의 식에 반환값 수식을 대입하여, 앞으로 받을 보상에 대한 기대값인 가치함수를 표현함

> $v(s) = E[R_{t+1} + \gamma R_{t+2} + \gamma ^2 R_{t+3}... |S_t = s]$
$\gamma R_{t+2}$부터 뒤의 항을 $\gamma$로 묶어주고 그것을 반환값의 형태로 표현 <br>
$ v(s) = E[R_{t+1} + \gamma (R_{t+2}+\gamma R_{t+3}...)|S_t = s]$
$v(s) = E[R_{t+1} + \gamma G_{t + 1}|S_t = s]$
$R_{t+2} + \gamma R_{t+3}...$ 부분은 에이저트가 실제로 받은 보상이 아니고 , **앞으로 받을 것 이라 예상하는 보상** 따라서 이부분을 앞으로 받을 보상에 대한 기대값인 가치함수로 표현 가능.<br>
_가치함수로 표현하는 가치함수의 정의_
$ v(s) = E[R_{t+1} + \gamma v(S_{t+1})|S_t = s]$
E = 기댓값 , $R_{t+1}$ = t일때 보상 , $\gamma v (S_{t+1})$ = t +1의 상태에 할인율 적용한 값<br>
여기까지는 가치함수를 정의할 때 정책을 고려하지 않지만, 에이전트가 앞으로 받을 보상에 대해 생각할 때 정책을 고려하지 않으면 안됨. **왜냐하면 상태에서 상태로 넘어갈 때 에이전트는 무조건 행동을 해야 하고 각 상태에서 행동을 하는 것이 에이전트의 정책이기 떄문임**

MDP로 정의되는 문제에서 가치함수는 항상 정책에 의존하게 됨 정책을 고려한 가치함수는 아래와 같이 표현됨

> $ v_\pi(s) = E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1})| S_t = s$
위 식은 강화학습에서 상당히 중요한 벨만 기대 방정식 (Bellman Expectation Equation)임 **현재 상태의 가치함수 $v_\pi(s)$와 다음상태의 가치함수$v_\pi(S_{t+1})$사이의 관계를 말해주는 방정식.

#### 큐함수

각 행동에 대해 가치를 알려주는 함수. 즉 어떤 상태에서 어떤 행동이 얼마나 좋은지 알려주는 함수를 행동 가치 함수라고하며 이를 **Q함수** 라고 부름
 
 <br>

큐함수는 2개의 행동 상태에서 따로 가치함수를 계산할 수 있음, 즉 상태 , 행동이라는 두가지 변수를 가지며 $q_\pi(s,a)$라고 나타냄

> 가치함수와 큐 함수사이의 관계 
1.각 행동을 했을 때 앞으로 받을 보상인 큐함수$q_\pi(s,a)$ 를 $\pi (a|s)$ 에 곱함<br>
2.모든 행동에 대해 큐함수와 $\pi (a|s)$를 곱한 값을 더하면 가치함수가 됨
$ v_\pi(s) = \sum_{a \in A} \pi (a|s)q_\pi(s,a)$

큐함수는 에이전트가 행동을 선택하는 기준을 정할때 주로 사용되며 큐함수 또한 벨만의 기대방정식의 형태로 나타낼 수 있음.<br> 가치함수의 식과 다른점은 조건문에 행동이 더 들어간다는 점임.

> 벨만방정식
 $q_\pi(s,a) = E_\pi[R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1})|S_t = s , A_t = a]$

